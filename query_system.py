#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SystÃ¨me de requÃªtes pour les collections Qdrant d'IT SPIRIT
Ce systÃ¨me permet d'interroger les collections Qdrant selon les prioritÃ©s dÃ©finies
et de formater les rÃ©ponses selon les formats demandÃ©s.
"""

import csv
import os
import re   
import json
import openai
import unicodedata
from fuzzywuzzy import fuzz 
from datetime import datetime, timedelta
from typing import List, Dict, Any
from openai import OpenAI
from dotenv import load_dotenv
from qdrant_client.http.models import FieldCondition, MatchValue, Range, Filter
from qdrant_client import QdrantClient
from time import time
from cache_manager import CacheManager

# Chargement des variables d'environnement
load_dotenv()
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# DÃ©finition du prompt systÃ¨me pour OpenAI
system_prompt = """
Tu es un assistant expert dans lâ€™analyse de requÃªtes utilisateur pour le moteur de recherche IT SPIRIT. Ton rÃ´le est de transformer ces requÃªtes en instructions structurÃ©es, selon les rÃ¨gles suivantes :

### RÃ¨gles de sÃ©lection des collections :
1. Si la requÃªte contient les mots "ticket" ou "incident" :
   â†’ Utiliser les collections : JIRA, CONFLUENCE, ZENDESK

2. Si la requÃªte mentionne "NetSuite" ou fait rÃ©fÃ©rence Ã  cet ERP :
   â†’ Utiliser : NETSUITE, NETSUITE_DUMMIES
   â†’ Ajouter : JIRA, CONFLUENCE, ZENDESK avec un filtre `erp = "NetSuite"`
   â†’ Si un client est prÃ©cisÃ©, inclure les collections associÃ©es Ã  ce client

3. Si la requÃªte mentionne "SAP" ou concerne cet ERP :
   â†’ Utiliser : SAP (et potentiellement JIRA, CONFLUENCE, ZENDESK)
   â†’ Si un client est prÃ©cisÃ©, inclure les collections associÃ©es Ã  ce client

4. Si la requÃªte mentionne explicitement une collection :
   - "ZENDESK" ou "Zendesk" â†’ Ajouter ZENDESK
   - "CONFLUENCE" ou "Confluence" â†’ Ajouter CONFLUENCE
   - "JIRA" ou "Jira" â†’ Ajouter JIRA
   â†’ Si un client est prÃ©cisÃ©, inclure les collections associÃ©es

### Format de sortie :
Tu dois retourner un JSON brut, sans balises Markdown ni code. Ce JSON doit contenir :
- `collections`: Liste des collections Ã  interroger (parmi : JIRA, CONFLUENCE, ZENDESK, NETSUITE, NETSUITE_DUMMIES, SAP)
- `filters`: Dictionnaire de filtres (par ex. `client`, `erp`, `date`, etc.)
- `use_embedding`: BoolÃ©en indiquant si la recherche utilise lâ€™embedding
- `limit`: Nombre de rÃ©sultats Ã  retourner

### Consignes spÃ©cifiques :
- Si la requÃªte concerne des sujets gÃ©nÃ©riques liÃ©s Ã  un ERP (ex : "configurer un module NetSuite"), ajoute un filtre `erp` avec "NetSuite" ou "SAP"
- Si la requÃªte contient des termes vagues comme "tickets rÃ©cents", ajoute un filtre de date au format : `{"date": {"gte": <timestamp il y a 6 mois>}}`
"""

# DÃ©finition des collections
COLLECTIONS = ["JIRA", "CONFLUENCE", "ZENDESK", "NETSUITE", "NETSUITE_DUMMIES", "SAP"]

# DÃ©finition des formats de rÃ©ponse
FORMATS = ["Summary", "Detail", "Guide"]

def normalize_string(text: str) -> str:
    if not text:
        return ""
    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')
    return text.upper().strip()

def extract_client_name_from_csv(query: str, csv_path: str = "ListeClients.csv"):
    """
    DÃ©tecte un nom de client dans une requÃªte utilisateur, basÃ© sur ListeClients.csv
    Retourne: (nom_client, score, source)
    """
    if not query or len(query.strip()) < 2:
        return None, 0.0, {}

    query_normalized = normalize_string(query)
    best_match = None
    best_score = 0
    best_source = None

    try:
        with open(csv_path, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f, delimiter=';')
            for row in reader:
                client_name = row.get("Client", "").strip()
                if not client_name:
                    continue

                # Match exact
                if normalize_string(client_name) in query_normalized:
                    return client_name, 100.0, {"source": client_name}

                # Match flou
                score = fuzz.ratio(query_normalized, normalize_string(client_name))
                if score > best_score:
                    best_score = score
                    best_match = client_name
                    best_source = client_name

        if best_match and best_score >= 80:
            return best_match, best_score, {"source": best_source}

    except Exception as e:
        print(f"[âš ï¸] Erreur lors de la dÃ©tection client depuis CSV: {e}")

    return None, 0.0, {}

def extract_json(text: str) -> str:
    match = re.search(r"\{[\s\S]*\}", text)
    return match.group(0) if match else text
def call_openai_assistant(erp: str, query: str) -> str:
    assistant_id = None
    if erp and "netsuite" in erp.lower():
        assistant_id = os.getenv("ASSISTANT_ID_NETSUITE")
    elif erp and "sap" in erp.lower():
        assistant_id = os.getenv("ASSISTANT_ID_SAP")


    if not assistant_id:
        return "âš ï¸ Aucun assistant configurÃ© pour cet ERP."

    try:
        thread = openai.beta.threads.create()
        openai.beta.threads.messages.create(
            thread_id=thread.id,
            role="user",
            content=query
        )

        run = openai.beta.threads.runs.create(
            thread_id=thread.id,
            assistant_id=assistant_id
        )

        while True:
            status = openai.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)
            if status.status == "completed":
                break
            elif status.status in ["failed", "cancelled"]:
                return f"âŒ Assistant {erp} : Ã©chec ({status.status})"
            time.sleep(1)

        messages = openai.beta.threads.messages.list(thread_id=thread.id)
        response = messages.data[0].content[0].text.value
        openai.beta.threads.delete(thread_id=thread.id)
        return response

    except Exception as e:
        return f"âŒ Erreur assistant {erp} : {str(e)}"
class QdrantSystem:
    """SystÃ¨me de requÃªtes pour les collections Qdrant d'IT SPIRIT"""
    
    def __init__(self, clients_file: str):
        """
        Initialise le systÃ¨me de requÃªtes
        
        Args:
            clients_file: Chemin vers le fichier CSV contenant les informations clients
        """
        self.clients = self._load_clients(clients_file)
        self.collections = COLLECTIONS
        self.formats = FORMATS
        self.client = QdrantClient(
            url=os.getenv("QDRANT_URL"),
            api_key=os.getenv("QDRANT_API_KEY")
        )
        self.cache = CacheManager(
            db_url=os.getenv("DATABASE_URL"),
            redis_url=os.getenv("REDIS_URL", "redis://localhost:6379/0")
        )
        print("[ğŸ”§] REDIS_URL chargÃ© :", os.getenv("REDIS_URL"))

    def enrich_query_with_openai(self, user_query):
        """
        Enrichit une requÃªte utilisateur en utilisant l'API OpenAI et en ajoutant des filtres locaux.

        Args:
            user_query (str): La requÃªte utilisateur Ã  enrichir.

        Returns:
            dict: La requÃªte enrichie sous forme de dictionnaire JSON.
        """
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ],
            temperature=0.0,
            max_tokens=500
        )

        enriched_query = response.choices[0].message.content.strip()
        enriched_json = json.loads(extract_json(enriched_query))

        filters = enriched_json.get("filters", {})
        # ğŸ” Si client dÃ©tectÃ© mais pas d'ERP, on complÃ¨te avec l'ERP du client
        if not filters.get("erp") and filters.get("client"):
            client_erp = self.get_client_erp(filters["client"])
            if client_erp:
                filters["erp"] = client_erp
                enriched_json["filters"] = filters

        query_upper = user_query.upper()

        if "date" not in filters and any(w in query_upper for w in ["TICKET", "RÃ‰CENT", "RÃ‰CENTS", "RECENT"]):
            six_months_ago = int(time()) - 60*60*24*180
            filters["date"] = {"gte": six_months_ago}
            enriched_json["filters"] = filters

        if not filters.get("client"):
            detected_client, score, _ = extract_client_name_from_csv(user_query)
            if detected_client:
                filters["client"] = detected_client
                enriched_json["filters"] = filters

        print("[ğŸ§  GPT - Query enrichie]", json.dumps(enriched_json, indent=2))
        return enriched_json
    
    def _load_clients(self, clients_file: str) -> Dict[str, Dict[str, Any]]:
        """
        Charge les informations clients depuis le fichier CSV
        
        Args:
            clients_file: Chemin vers le fichier CSV contenant les informations clients
            
        Returns:
            Dictionnaire des clients avec leurs informations
        """
        clients = {}
        
        with open(clients_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f, delimiter=';')
            for row in reader:
                client_name = row['Client']
                if client_name not in clients:
                    clients[client_name] = {
                        'consultant': row['Consultant'],
                        'statut': row['Statut'],
                        'jira': row['JIRA'],
                        'zendesk': row['ZENDESK'],
                        'confluence': row['CONFLUENCE'],
                        'erp': row['ERP']
                    }
                # Si le client existe dÃ©jÃ , on ne met Ã  jour que les champs vides
                else:
                    for field, csv_field in [('consultant', 'Consultant'), ('statut', 'Statut'), 
                                           ('jira', 'JIRA'), ('zendesk', 'ZENDESK'), 
                                           ('confluence', 'CONFLUENCE'), ('erp', 'ERP')]:
                        if not clients[client_name][field] and row[csv_field]:
                            clients[client_name][field] = row[csv_field]
        
        return clients
    def simple_filter_search(self, collection_name, client_name=None, recent_only=False, filters: Filter = None, limit=5):
        """
        Effectue une recherche simple dans une collection Qdrant sans vectorisation.

        Args:
            collection_name: Nom de la collection Qdrant
            client_name: Nom du client pour le filtrage (optionnel)
            recent_only: Si True, filtre les rÃ©sultats pour les Ã©lÃ©ments crÃ©Ã©s il y a moins de 6 mois (optionnel)
            filters: Objet Filter Qdrant Ã  appliquer (filtrage par client, date, etc.) (optionnel)
            limit: Nombre de rÃ©sultats Ã  retourner

        Returns:
            Liste des payloads formatÃ©s
        """
        filter_conditions = []

        if client_name:
            filter_conditions.append(
                FieldCondition(
                    key="client",
                    match=MatchValue(value=client_name)
                )
            )

        if recent_only:
            six_months_ago = int((datetime.now() - timedelta(days=180)).timestamp())
            filter_conditions.append(
                FieldCondition(
                    key="created",
                    range=Range(gte=six_months_ago)
                )
            )

        if filters:
            search_filter = filters
        else:
            search_filter = Filter(must=filter_conditions) if filter_conditions else None

        results, _ = self.client.scroll(
            collection_name=collection_name,
            scroll_filter=search_filter,
            limit=limit,
            with_payload=True
        )

        return [self.format_ticket_payload(hit.payload) for hit in results]

    def search_in_collection(self, collection_name: str, query: str, client_name: str = None, recent_only: bool = False, limit: int = 5, filters: Filter = None):
        """
        Effectue une recherche dans une collection avec vectorisation et filtres.

        Args:
            collection_name: Nom de la collection
            query: Texte de la requÃªte utilisateur
            client_name: Nom du client (optionnel, redondant avec filters)
            recent_only: BoolÃ©en pour filtrer les donnÃ©es rÃ©centes (non utilisÃ© ici)
            limit: Nombre de rÃ©sultats Ã  retourner
            filters: Filtre Qdrant (dÃ©jÃ  construit via enrich_query_with_openai)

        Returns:
            Liste des documents pertinents (payloads) avec leurs scores
        """
        embedding_response = openai_client.embeddings.create(
            input=query,
            model="text-embedding-ada-002"
        )
        query_vector = embedding_response.data[0].embedding

        results = self.client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            query_filter=filters,
            limit=limit,
            with_payload=True
        )

        return [(hit.payload, hit.score) for hit in results]

    def get_client_erp(self, client_name: str) -> str:
        """
        RÃ©cupÃ¨re le systÃ¨me ERP utilisÃ© par un client
        
        Args:
            client_name: Nom du client
            
        Returns:
            SystÃ¨me ERP utilisÃ© par le client (SAP ou NetSuite)
        """
        if client_name in self.clients:
            return self.clients[client_name]['erp']
        return ""
    
    def get_client_info(self, client_name: str) -> Dict[str, Any]:
        """
        RÃ©cupÃ¨re les informations d'un client
        
        Args:
            client_name: Nom du client
            
        Returns:
            Informations du client
        """
        if client_name in self.clients:
            return self.clients[client_name]
        return {}
    
    def get_prioritized_collections(self, client_name: str = "", erp: str = "") -> List[str]:
        """
        RÃ©cupÃ¨re les collections prioritaires selon le client et le systÃ¨me ERP
        
        Args:
            client_name: Nom du client (optionnel)
            erp: SystÃ¨me ERP (optionnel)
            
        Returns:
            Liste des collections prioritaires
        """
        # Si on a un client, on priorise JIRA, CONFLUENCE, ZENDESK
        if client_name:
            if erp == "SAP":
                return ["JIRA", "CONFLUENCE", "ZENDESK", "SAP"]
            elif erp == "NetSuite":
                return ["JIRA", "CONFLUENCE", "ZENDESK", "NETSUITE", "NETSUITE_DUMMIES"]
            else:
                # Si on ne connaÃ®t pas l'ERP, on vÃ©rifie dans les donnÃ©es client
                client_erp = self.get_client_erp(client_name)
                if client_erp == "SAP":
                    return ["JIRA", "CONFLUENCE", "ZENDESK", "SAP"]
                elif client_erp == "NetSuite":
                    return ["JIRA", "CONFLUENCE", "ZENDESK", "NETSUITE", "NETSUITE_DUMMIES"]
                else:
                    # Si on ne trouve toujours pas l'ERP, on renvoie toutes les collections
                    return ["JIRA", "CONFLUENCE", "ZENDESK", "SAP", "NETSUITE", "NETSUITE_DUMMIES"]
        
        # Si on a un ERP mais pas de client
        if erp == "SAP":
            return ["SAP", "JIRA", "CONFLUENCE", "ZENDESK"]
        elif erp == "NetSuite":
            return ["NETSUITE", "NETSUITE_DUMMIES", "JIRA", "CONFLUENCE", "ZENDESK"]
        
        # Si on n'a ni client ni ERP, on renvoie toutes les collections
        return ["JIRA", "CONFLUENCE", "ZENDESK", "SAP", "NETSUITE", "NETSUITE_DUMMIES"]
    
    def is_recent(self, date_str: str) -> bool:
        """
        VÃ©rifie si une date est rÃ©cente (moins de 6 mois)
        
        Args:
            date_str: Date au format string
            
        Returns:
            True si la date est rÃ©cente, False sinon
        """
        try:
            date = datetime.strptime(date_str, "%Y-%m-%d")
            six_months_ago = datetime.now() - timedelta(days=180)
            return date >= six_months_ago
        except (ValueError, TypeError):
            return False

    def format_ticket_payload(self, payload: dict, score: float = None, format_type: str = "Detail") -> dict:
        def get_score_color(score):
            if score is None:
                return "gray"
            if score >= 0.80:
                return "green"
            elif score >= 0.50:
                return "orange"
            else:
                return "red"

        def format_timestamp(ts):
            try:
                if isinstance(ts, (int, float)):
                    return datetime.fromtimestamp(ts).strftime('%d/%m/%Y')
                elif isinstance(ts, str) and ts.isdigit():
                    return datetime.fromtimestamp(int(ts)).strftime('%d/%m/%Y')
                return ts
            except Exception:
                return ts

        base = {
            "client": payload.get("client", "N/A"),
            "erp": payload.get("erp", "N/A"),
            "summary": payload.get("summary", payload.get("description", "")),
            "description": payload.get("description", ""),
            "content": payload.get("content", ""),
            "comments": payload.get("comments", ""),
            "created": format_timestamp(payload.get("created", "N/A")),
            "updated": format_timestamp(payload.get("updated", "N/A")),
            "assignee": payload.get("assignee", "N/A"),
            "source": payload.get("source_type", "N/A"),
            "key": payload.get("key", None),
            "url": payload.get("url") or payload.get("page_url"),
            "company_name": payload.get("company_name", None),
            "score": round(score, 4) if score is not None else None,
            "color": get_score_color(score),
        }

        if format_type == "Guide":
            content_parts = []
            for key in ["summary", "description", "content", "text", "comments"]:
                val = payload.get(key)
                if val:
                    content_parts.append(str(val))
            base["content"] = "\n".join(content_parts)

        return base

    def format_response(self, content: Dict[str, Any], format_type: str = "Summary") -> str:
        """
        Formate la rÃ©ponse selon le format demandÃ©
        
        Args:
            content: Contenu de la rÃ©ponse
            format_type: Format de la rÃ©ponse (Summary, Detail, Guide)
            
        Returns:
            RÃ©ponse formatÃ©e
        """
        if format_type not in self.formats:
            format_type = "Summary"
        
        # Formatage selon le type demandÃ©
        if format_type == "Summary":
            # RÃ©sumÃ© bref
            return self._format_summary(content)
        elif format_type == "Detail":
            # Explication complÃ¨te
            return self._format_detail(content)
        elif format_type == "Guide":
            # Instructions Ã©tape par Ã©tape
            return self._format_guide(content)
    @staticmethod
    def convert_to_timestamp(date_str):
        if isinstance(date_str, (int, float)):
            return date_str
        try:
            return int(datetime.fromisoformat(date_str.replace("Z", "+00:00")).timestamp())
        except Exception:
            return None  # ou raise une erreur selon ton choix

    def apply_filters(self, filters: dict) -> Filter:
        """
        Transforme un dictionnaire de filtres en objet Filter pour Qdrant

        Args:
            filters: Dictionnaire de filtres (client, erp, date, etc.)

        Returns:
            Objet Filter compatible avec Qdrant
        """
        conditions = []

        if "client" in filters:
            conditions.append(
                FieldCondition(key="client", match=MatchValue(value=filters["client"]))
            )

        if "erp" in filters:
            conditions.append(
                FieldCondition(key="erp", match=MatchValue(value=filters["erp"]))
            )

        if "date" in filters:
            date_filter = filters["date"]
            if isinstance(date_filter, dict):
                gte = date_filter.get("gte")
                lte = date_filter.get("lte")
                range_filter = {}
                if gte:
                    range_filter["gte"] = QdrantSystem.convert_to_timestamp(gte)
                if lte:
                    range_filter["lte"] = QdrantSystem.convert_to_timestamp(lte)
                if range_filter:  # Only add the condition if we have date filters
                    conditions.append(FieldCondition(key="created", range=Range(**range_filter)))


        return Filter(must=conditions) if conditions else None

    def _format_summary(self, content: Dict[str, Any]) -> str:
        """
        Formate la rÃ©ponse en rÃ©sumÃ© bref
        
        Args:
            content: Contenu de la rÃ©ponse
            
        Returns:
            RÃ©sumÃ© bref
        """
        # ImplÃ©mentation du formatage en rÃ©sumÃ©
        summary = content.get("summary", "")
        if not summary and "content" in content:
            # Si pas de rÃ©sumÃ© mais du contenu, on prend les 200 premiers caractÃ¨res
            summary = content["content"][:200] + "..." if len(content["content"]) > 200 else content["content"]
        
        return summary
    
    def _format_detail(self, content: Dict[str, Any]) -> str:
        """
        Formate la rÃ©ponse en explication complÃ¨te
        
        Args:
            content: Contenu de la rÃ©ponse
            
        Returns:
            Explication complÃ¨te
        """
        # ImplÃ©mentation du formatage en dÃ©tail
        detail = ""
        
        # Titre ou rÃ©sumÃ©
        if "title" in content:
            detail += f"## {content['title']}\n\n"
        elif "summary" in content:
            detail += f"## {content['summary']}\n\n"
        
        # Contenu principal
        if "content" in content:
            detail += f"{content['content']}\n\n"
        elif "description" in content:
            detail += f"{content['description']}\n\n"
        elif "text" in content:
            detail += f"{content['text']}\n\n"
        
        # Informations supplÃ©mentaires
        if "comments" in content and content["comments"]:
            detail += f"### Commentaires\n{content['comments']}\n\n"
        # Ajouter le score
        if "score" in content:
            detail += f"Score de similaritÃ© : {content['score']}\n\n"
        # MÃ©tadonnÃ©es
        metadata = []
        if "created" in content:
            metadata.append(f"CrÃ©Ã© le: {content['created']}")
        if "updated" in content:
            metadata.append(f"Mis Ã  jour le: {content['updated']}")
        if "assignee" in content:
            metadata.append(f"AssignÃ© Ã : {content['assignee']}")
        
        if metadata:
            detail += "---\n" + "\n".join(metadata)
        
        return detail
    
    def _format_guide(self, content: Dict[str, Any]) -> str:
        """
        Formate la rÃ©ponse en instructions Ã©tape par Ã©tape
        
        Args:
            content: Contenu de la rÃ©ponse
            
        Returns:
            Instructions Ã©tape par Ã©tape
        """
        # ImplÃ©mentation du formatage en guide
        guide = ""
        
        # Titre
        if "title" in content:
            guide += f"# Guide: {content['title']}\n\n"
        elif "summary" in content:
            guide += f"# Guide: {content['summary']}\n\n"
        
        # Introduction
        if "description" in content:
            guide += f"## Introduction\n{content['description']}\n\n"
        
        # Contenu principal - on essaie de le structurer en Ã©tapes
        if "content" in content:
            # On tente de diviser le contenu en Ã©tapes
            steps = self._extract_steps(content["content"])
            if steps:
                guide += "## Ã‰tapes Ã  suivre\n\n"
                for i, step in enumerate(steps, 1):
                    guide += f"{i}. {step}\n"
            else:
                guide += f"## ProcÃ©dure\n{content['content']}\n\n"
        elif "text" in content:
            steps = self._extract_steps(content["text"])
            if steps:
                guide += "## Ã‰tapes Ã  suivre\n\n"
                for i, step in enumerate(steps, 1):
                    guide += f"{i}. {step}\n"
            else:
                guide += f"## ProcÃ©dure\n{content['text']}\n\n"
        
        # Notes ou conseils
        if "comments" in content and content["comments"]:
            guide += f"\n## Notes et conseils\n{content['comments']}\n"
        
        return guide
    
    def _extract_steps(self, text: str) -> List[str]:
        """
        Extrait les Ã©tapes d'un texte
        
        Args:
            text: Texte Ã  analyser
            
        Returns:
            Liste des Ã©tapes extraites
        """
        # Recherche de patterns comme "1.", "Step 1:", "Ã‰tape 1:", etc.
        steps = []
        lines = text.split("\n")
        
        # Patterns possibles pour les Ã©tapes
        step_patterns = [
            r"^\d+\.",  # "1."
            r"^Step \d+:",  # "Step 1:"
            r"^Ã‰tape \d+:",  # "Ã‰tape 1:"
            r"^Ã‰tape \d+\.",  # "Ã‰tape 1."
        ]
        
        current_step = ""
        in_step = False
        
        for line in lines:
            is_step_header = any(pattern in line for pattern in step_patterns)
            
            if is_step_header:
                if in_step and current_step:
                    steps.append(current_step.strip())
                current_step = line
                in_step = True
            elif in_step:
                current_step += " " + line
        
        # Ajouter la derniÃ¨re Ã©tape
        if in_step and current_step:
            steps.append(current_step.strip())
        
        # Si on n'a pas trouvÃ© d'Ã©tapes avec les patterns, on essaie de diviser le texte
        if not steps and len(lines) > 3:
            # On divise le texte en paragraphes et on prend chaque paragraphe comme une Ã©tape
            paragraphs = []
            current_para = ""
            
            for line in lines:
                if line.strip():
                    current_para += line + " "
                elif current_para:
                    paragraphs.append(current_para.strip())
                    current_para = ""
            
            if current_para:
                paragraphs.append(current_para.strip())
            
            # Si on a entre 3 et 10 paragraphes, on les considÃ¨re comme des Ã©tapes
            if 3 <= len(paragraphs) <= 10:
                steps = paragraphs
        
        return steps
    
    def create_response(self, query_result: Dict[str, Any], format_type: str = "Summary", collections_used: List[str] = None) -> Dict[str, str]:
        """
        CrÃ©e la rÃ©ponse finale au format demandÃ©
        
        Args:
            query_result: RÃ©sultat de la requÃªte
            format_type: Format de la rÃ©ponse (Summary, Detail, Guide)
            collections_used: Collections utilisÃ©es pour la requÃªte
            
        Returns:
            RÃ©ponse formatÃ©e selon le template demandÃ©
        """
        if not collections_used:
            collections_used = []
        
        content = self.format_response(query_result, format_type)
        
        return {
            "format": format_type,
            "content": content,
            "sources": ", ".join(collections_used)
        }
    def is_response_useless(self, text: str) -> bool:
        """
        DÃ©tecte si la rÃ©ponse GPT est inutile ou trop vague
        """
        useless_patterns = [
            "je ne peux pas rÃ©pondre", 
            "je ne suis pas sÃ»r", 
            "je ne dispose pas", 
            "je n'ai pas assez d'informations", 
            "donnez plus de dÃ©tails", 
            "veuillez prÃ©ciser",
            "je suis un assistant", 
            "en tant qu'assistant", 
            "je ne suis pas capable"
        ]
        lower_text = text.lower()
        return any(p in lower_text for p in useless_patterns)

    def process_query(self, query, client_name=None, erp=None, recent_only=False, limit=5, format_type="Summary", raw=False, deepresearch=None):
    
        USE_EMBEDDING = os.getenv("USE_EMBEDDING", "true").lower() == "true"

        # Ã‰tape 1 : enrichissement de la requÃªte via GPT
        enriched_query = self.enrich_query_with_openai(query)

        # ğŸ” Activation automatique de use_embedding si la requÃªte parle de tickets/incidents
        trigger_keywords = ["ticket", "incident", "erreur", "problÃ¨me", "anomalie", "crash", "panne"]
        if any(kw in query.lower() for kw in trigger_keywords):
            enriched_query["use_embedding"] = True
        else:
            enriched_query["use_embedding"] = False
        filters = enriched_query.get("filters", {})
        erp = erp or filters.get("erp")

        # ğŸ’¡ PATCH : Si la requÃªte contient "ticket" mais pas dâ€™ERP â†’ on dÃ©sactive deepresearch
        if deepresearch and not erp and "ticket" in query.lower():
            print("ğŸ”• Aucun ERP dÃ©tectÃ© â†’ Assistant GPT spÃ©cialisÃ© dÃ©sactivÃ© pour cette requÃªte.")
            deepresearch = False

        # ğŸ” Activation automatique de deepresearch pour les questions fonctionnelles
        deepresearch = True
        # ğŸ” RÃ©cupÃ©ration dynamique de l'ERP depuis les filtres enrichis
        if not erp:
            erp = enriched_query.get("filters", {}).get("erp")
        # Ã‰tape 1bis : vÃ©rification de la qualitÃ© de la question
        if len(query.strip()) < 10:
            raise ValueError("âŒ La question est trop courte pour une analyse pertinente.")
        # âŒ Blocage explicite du format Guide si la requÃªte est trop vague
        if format_type == "Guide" and "ticket" in query.lower():
            guide_keywords = ["comment", "configurer", "rÃ©soudre", "procÃ©dure", "Ã©tapes", "corriger", "bug", "erreur", "solution"]
            if not any(kw in query.lower() for kw in guide_keywords):
                print("âš ï¸ RequÃªte trop vague pour un guide. Passage automatique en mode 'Detail'.")
                format_type = "Detail"

        # Ã‰tape 2 : clÃ© de cache
        filters_dict = enriched_query.get("filters", {})
        cache_key = self.cache.compute_key(query, filters_dict, limit)
        format_key = f"{format_type.upper()}:{cache_key}"

        print("\nğŸ§  [DEBUG] âœ cache_key:", cache_key)
        print("ğŸ§  [DEBUG] âœ format_key:", format_key)

        all_results = self.cache.get_raw_results(cache_key)
        client_erp = self.get_client_erp(filters.get("client")) if filters.get("client") else None
        if not all_results:
            # Ã‰tape 3 : recherche dans Qdrant
            collections = enriched_query.get("collections") or self.get_prioritized_collections(client_name, erp)
            filters = self.apply_filters(filters_dict)
            limit = enriched_query.get("limit", limit)
            use_embedding = enriched_query.get("use_embedding", USE_EMBEDDING)

            all_results = []
            for collection_name in collections:
                try:
                    remaining = limit - len(all_results)
                    if remaining <= 0:
                        break
                    
                    if use_embedding:
                        results = self.search_in_collection(collection_name, query, client_name, recent_only, remaining, filters)
                        all_results.extend([
                            self.format_ticket_payload(payload, score, format_type)
                            for payload, score in results
                        ])
                    else:
                        raw_results = self.simple_filter_search(collection_name, client_name, recent_only, filters, remaining)
                        all_results.extend([
                            self.format_ticket_payload(payload, format_type=format_type)
                            for payload in raw_results
                        ])
                except Exception as e:
                    print(f"Erreur dans la collection {collection_name}: {str(e)}")

            all_results.sort(key=lambda r: r.get("created", ""), reverse=True)
            self.cache.store_raw_results(cache_key, query, filters_dict, limit, use_embedding, all_results)

        # Ã‰tape 4 : vÃ©rifie si format dÃ©jÃ  calculÃ©
        # ğŸ§© Patch : si format Detail + rÃ©sultats trouvÃ©s â†’ pas de GPT, on retourne les payloads
        if format_type == "Detail" and not deepresearch and all_results:
            return {
                "format": format_type,
                "content": all_results[:limit],
                "sources": ", ".join(collections),
                "meta": {
                    "erp": filters_dict.get("erp") or client_erp,
                    "dateFilter": filters_dict.get("date"),
                    "use_embedding": use_embedding
                }
            }

        format_key = f"{format_type.upper()}:{cache_key}"
        cached_format = self.cache.get_format(format_key)

        if cached_format:
            print("âœ… Format rÃ©cupÃ©rÃ© depuis le cache :", format_key)
            return {
                "format": format_type,
                "content": [cached_format["content"]] if isinstance(cached_format["content"], str) else cached_format["content"],
                "sources": cached_format["sources"],
                "meta": cached_format.get("meta", {})
            }
        else:
            print("ğŸš« [CACHE] Aucun format trouvÃ© â†’ on continue.")    
        # Ã‰tape 5 : deepresearch â†’ GPT spÃ©cialisÃ© + fusion
        if deepresearch:
            # Si le format est Summary et deepresearch est activÃ©, traitement spÃ©cifique
            if format_type == "Summary":
                summaries = "\n".join(r.get("summary", "") for r in all_results[:limit])
                specialist_response = call_openai_assistant(erp, query)
                erp_label = client_erp or filters_dict.get("erp") or "l'ERP concernÃ©"
                fusion_prompt = f"""Voici deux sources d'information sur la question suivante : "{query}"

                    1. RÃ©ponse du spÃ©cialiste ERP :
                    {specialist_response}

                    2. RÃ©sumÃ© de tickets internes :
                    {summaries}

                    Fais une synthÃ¨se claire, complÃ¨te et utile pour un utilisateur de {erp_label}."""

                response = openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "Tu es un assistant qui produit des synthÃ¨ses enrichies Ã  partir de sources internes et externes."},
                        {"role": "user", "content": fusion_prompt}
                    ],
                    temperature=0.3,
                    max_tokens=1000
                )

                summary_text = response.choices[0].message.content.strip()
                self.cache.store_format(format_key, "Summary", summary_text, ", ".join(collections), {
                    "erp": filters_dict.get("erp") or client_erp,
                    "dateFilter": filters_dict.get("date"),
                    "mode": "deepresearch"
                })
                return {
                    "format": format_type,
                    "content": [summary_text],
                    "sources": ", ".join(collections),
                    "meta": {
                        "erp": filters_dict.get("erp") or client_erp,
                        "dateFilter": filters_dict.get("date"),
                        "mode": "deepresearch",
                        "use_embedding": use_embedding
                    }
                }
            # Sinon, traitement gÃ©nÃ©rique pour les autres formats avec deepresearch
            else:
                specialist_response = call_openai_assistant(erp, query)
                summaries = "\n".join(r.get("summary", "") for r in all_results[:limit])
                fusion_prompt = f"""RÃ©ponds Ã  la question suivante Ã  partir de deux sources complÃ©mentaires :\n\n1. RÃ©ponse du spÃ©cialiste ERP :\n\n{specialist_response}\n\n2. DonnÃ©es internes extraites des tickets et documentations :\n\n{summaries}\n\nRÃ©dige une rÃ©ponse enrichie, claire et utile, en combinant les deux."""

                gpt_fused = openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "Tu combines les connaissances gÃ©nÃ©rales et les donnÃ©es internes pour produire une rÃ©ponse enrichie."},
                        {"role": "user", "content": fusion_prompt}
                    ],
                    temperature=0.3,
                    max_tokens=1000
                ).choices[0].message.content.strip()

                return {
                    "format": format_type,
                    "content": [gpt_fused],
                    "sources": ", ".join(collections),
                    "meta": {
                        "erp": filters_dict.get("erp") or client_erp,
                        "dateFilter": filters_dict.get("date"),
                        "mode": "deepresearch",
                        "use_embedding": use_embedding
                    }
                }

        # Ã‰tape 6 : traitement format classique (Summary, Guide, Detail)
        if format_type == "Summary" and not deepresearch:
            joined_summaries = "\n".join(r.get("summary", "") for r in all_results[:limit])
            prompt = f"Voici une liste de tickets utilisateurs concernant : {query}\n\n{joined_summaries}\n\nFais-en un rÃ©sumÃ© clair et concis."
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Tu es un assistant expert en synthÃ¨se de tickets clients."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=300
            )
            summary_text = response.choices[0].message.content.strip()
            self.cache.store_format(format_key, "Summary", summary_text, ", ".join(collections), {
                "erp": filters_dict.get("erp") or client_erp,
                "dateFilter": filters_dict.get("date")
            })
            if self.is_response_useless(summary_text):
                return {
                "format": format_type,
                "content": ["âŒ Aucun rÃ©sumÃ© utile nâ€™a pu Ãªtre gÃ©nÃ©rÃ© Ã  partir des donnÃ©es internes ou du GPT spÃ©cialisÃ©."],
                "sources": ", ".join(collections),
                "meta": {
                    "erp": filters_dict.get("erp") or client_erp,
                    "dateFilter": filters_dict.get("date"),
                    "mode": "deepresearch",
                    "use_embedding": use_embedding
                }
            }
            return {
                "format": format_type,
                "content": [summary_text],
                "sources": ", ".join(collections),
                "meta": {
                    "erp": filters_dict.get("erp") or client_erp,
                    "dateFilter": filters_dict.get("date")
                }
            }

        elif format_type == "Guide":
            guide_input = "\n".join(r.get("summary", "") + "\n" + r.get("content", "") for r in all_results[:limit] if "content" in r)
            prompt = f"Voici des extraits de tickets. RÃ©dige un guide pratique en Ã©tapes pour rÃ©soudre le problÃ¨me Ã©voquÃ© :\n\n{guide_input}"
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Tu es un assistant qui transforme des contenus de tickets en guide Ã©tape par Ã©tape."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            guide_text = response.choices[0].message.content.strip()

            if self.is_response_useless(guide_text):
                return {
                    "format": format_type,
                    "content": ["âŒ Aucun guide utile nâ€™a pu Ãªtre gÃ©nÃ©rÃ© Ã  partir des donnÃ©es internes ou du GPT spÃ©cialisÃ©."],
                    "sources": ", ".join(collections),
                    "meta": {
                        "erp": filters_dict.get("erp") or client_erp,
                        "dateFilter": filters_dict.get("date"),
                        "mode": "deepresearch",
                        "use_embedding": use_embedding
                    }
                }

            self.cache.store_format(
                format_key,
                "Guide",
                [guide_text],
                ", ".join(collections),
                {
                    "erp": filters_dict.get("erp") or client_erp,
                    "dateFilter": filters_dict.get("date"),
                    "mode": "deepresearch"
                }
            )

            return {
                "format": format_type,
                "content": [guide_text],
                "sources": ", ".join(collections),
                "meta": {
                    "erp": filters_dict.get("erp") or client_erp,
                    "dateFilter": filters_dict.get("date")
                }
            }


        # Sinon, traitement gÃ©nÃ©rique pour les autres formats avec deepresearch
        else:
            specialist_response = call_openai_assistant(erp, query)
            summaries = "\n".join(r.get("summary", "") for r in all_results[:limit])
            fusion_prompt = f"""RÃ©ponds Ã  la question suivante Ã  partir de deux sources complÃ©mentaires :\n\n1. RÃ©ponse du spÃ©cialiste ERP :\n\n{specialist_response}\n\n2. DonnÃ©es internes extraites des tickets et documentations :\n\n{summaries}\n\nRÃ©dige une rÃ©ponse enrichie, claire et utile, en combinant les deux."""

            gpt_fused = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Tu combines les connaissances gÃ©nÃ©rales et les donnÃ©es internes pour produire une rÃ©ponse enrichie."},
                    {"role": "user", "content": fusion_prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            ).choices[0].message.content.strip()

            if format_type == "Detail" and not deepresearch:
                # Chargement des rÃ©sultats bruts (tickets)
                all_results = self.cache.get_raw_results(cache_key)
                if all_results:
                    print(f"âœ… [CACHE] RÃ©sultats Qdrant rÃ©cupÃ©rÃ©s depuis le cache : {len(all_results)} entrÃ©es.")
                    return {
                        "format": format_type,
                        "content": all_results[:limit],
                        "sources": ", ".join(collections),
                        "meta": {
                            "erp": filters_dict.get("erp") or client_erp,
                            "dateFilter": filters_dict.get("date")
                        }
                    }
                else:
                    print("ğŸš« [CACHE] Aucun ticket trouvÃ© dans les collections interrogÃ©es.")
                    return {
                        "format": format_type,
                        "content": ["âŒ Aucun ticket trouvÃ© dans les collections interrogÃ©es."],
                        "sources": ", ".join(collections),
                        "meta": {
                            "erp": filters_dict.get("erp") or client_erp,
                            "dateFilter": filters_dict.get("date")
                        }
                    }

            # Sinon (fallback enrichi avec GPT)
            specialist_response = call_openai_assistant(erp, query)
            summaries = "\n".join(r.get("summary", "") for r in all_results[:limit])
            fusion_prompt = f"""RÃ©ponds Ã  la question suivante Ã  partir de deux sources complÃ©mentaires :\n\n1. RÃ©ponse du spÃ©cialiste ERP :\n\n{specialist_response}\n\n2. DonnÃ©es internes extraites des tickets et documentations :\n\n{summaries}\n\nRÃ©dige une rÃ©ponse enrichie, claire et utile, en combinant les deux."""

            gpt_fused = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Tu combines les connaissances gÃ©nÃ©rales et les donnÃ©es internes pour produire une rÃ©ponse enrichie."},
                    {"role": "user", "content": fusion_prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            ).choices[0].message.content.strip()

            if format_type == "Detail":
                self.cache.store_format(
                    format_key,
                    "Detail",
                    [{
                        "summary": gpt_fused,
                        "created": None,
                        "updated": None,
                        "source": "GPT",
                        "assignee": None,
                        "url": None,
                        "score": None
                    }],
                    ", ".join(collections),
                    {
                        "erp": filters_dict.get("erp") or client_erp,
                        "dateFilter": filters_dict.get("date"),
                        "mode": "deepresearch"
                    }
                )

                return {
                    "format": format_type,
                    "content": [{
                        "summary": gpt_fused,
                        "created": None,
                        "updated": None,
                        "source": "GPT",
                        "assignee": None,
                        "url": None,
                        "score": None
                    }],
                    "sources": ", ".join(collections),
                    "meta": {
                        "erp": filters_dict.get("erp") or client_erp,
                        "dateFilter": filters_dict.get("date"),
                        "mode": "deepresearch",
                        "use_embedding": use_embedding
                    }
                }

            # Sinon (ex: Guide), retour classique texte enrichi
            return {
                "format": format_type,
                "content": [gpt_fused],
                "sources": ", ".join(collections),
                "meta": {
                    "erp": filters_dict.get("erp") or client_erp,
                    "dateFilter": filters_dict.get("date"),
                    "mode": "deepresearch",
                    "use_embedding": use_embedding
                }
            }





# Fonction principale pour tester le systÃ¨me
def main():
    """Fonction principale pour tester le systÃ¨me"""
    clients_file = "/home/ubuntu/upload/ListeClients.csv"
    system = QdrantSystem(clients_file)
    
    # Test avec un client SAP
    client_name = "AZERGO"
    print(f"Client: {client_name}")
    print(f"ERP: {system.get_client_erp(client_name)}")
    print(f"Collections prioritaires: {system.get_prioritized_collections(client_name)}")
    
    # Test avec un client NetSuite
    client_name = "ADVIGO"
    print(f"\nClient: {client_name}")
    print(f"ERP: {system.get_client_erp(client_name)}")
    print(f"Collections prioritaires: {system.get_prioritized_collections(client_name)}")
    
    # Test avec un ERP sans client
    erp = "NetSuite"
    print(f"\nERP: {erp}")
    print(f"Collections prioritaires: {system.get_prioritized_collections(erp=erp)}")

if __name__ == "__main__":
    main()
